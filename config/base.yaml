# Default or base configuration for SE(3) diffusion experiments.

defaults:
  - override hydra/launcher: joblib

data:
  # CSV for path and metadata to training examples.
  csv_path: ./data/processed_pdb/metadata.csv
  cluster: ./data/foldseekDB/pdbDB/pdb90.tsv
  filtering:
    max_len: 512
    min_len: 60
    # Selects a subset of examples. Useful for debugging.
    subset: null
    # monomer type
    #   monomer
    # complex type
    #   dimer(2),dimer with peptides(?),trimer(3),tetramer(4),pentamer(5),
    #   hexamer(6),heptamer(7),octamer(8),nonamer(9),decamer(10),undecamer(11),
    #   dodecamer(12),tridecamer(13),tetradecamer(14),pentadecamer(15),hexadecamer(16),
    #   heptadecamer(17),octadecamer(18),nonadecamer(19),eicosamer(20)'''
    allowed_oligomer: [monomer]
    max_helix_percent: 1.0
    max_loop_percent: 0.5
    min_beta_percent: -1.0
    rog_quantile: 0.96
  min_t: 0.01
  num_t: 200
  samples_per_eval_length: 4
  num_eval_lengths: 10
  train:
    # the delta t range between prev step and training step, the origin training setting can be viewed as delta_t_range = [1] in num_t = 100
    delta_t_range: [0]
    crop: False
    crop_size: 512
    # we only use template channel to embed self_condition feature
    max_templates: null
  eval:
    crop: False
    crop_size: null
    max_templates: null
  # config used in strcture prediction/ , not available here
  max_template_hits: 0
  max_templates: 0
  subsample_templates: False
  shuffle_top_k_prefiltered : 0

diffuser:
  diffuse_trans: True
  diffuse_rot: True

  # R(3) diffuser arguments
  r3:
    min_b: 0.1
    max_b: 20.0
    coordinate_scaling: 0.1

  # SO(3) diffuser arguments
  so3:
    num_omega: 1000
    num_sigma: 1000
    min_sigma: 0.1
    max_sigma: 1.5
    schedule: logarithmic
    cache_dir: .cache/

model:
  # an option for analysis memory and time for each block of model
  profile: False
  # control if sidechain prediction is trained
  sidechain : True
  node_embed_size: 256
  edge_embed_size: 128
  dropout: 0.0
  
  embed:
    feature:
      aatype: False
      index: False
      rel_pos: 32
      t: 32
      distogram:
        min_bin: 1e-5
        max_bin: 20.0
        no_bins: 22
      
    self_condition:
      # baseline: ca distogram, template: af2 template feature, null: no self-condition
      version: template
      # now aatype process has two way: mask(mask all residue identity),null: same as input,
      aatype: mask
      # backbone,all_atom
      all_atom_mask: backbone
    node_embed_size: ${model.node_embed_size}
    edge_embed_size: ${model.edge_embed_size}
    inf: 1e9
    eps: 1e-6

    template:
      c_s: ${model.node_embed_size}
      c_z: ${model.edge_embed_size}
      c_t: 64
      inf: 1e9
      eps: 1e-6

      distogram:
        min_bin: 3.25
        max_bin: 50.75
        no_bins: 39
      
      template_angle_embedder:
        # DISCREPANCY: c_in is supposed to be 51.
        c_in: 57
        c_out: ${model.node_embed_size}
      
      template_pair_embedder:
        c_in: 88
        c_out: ${model.embed.template.c_t}
      
      template_pair_stack:
        c_t: ${model.embed.template.c_t}
        c_hidden_tri_mul: 32
        pair_transition_n: 2
        dropout_rate: 0.25
        inf: 1e9

      template_cross_embedder:
        template_pointwise_attention:
          c_t: ${model.embed.template.c_t}
          c_z: ${model.edge_embed_size}
          c_hidden: 16
          no_heads: 4
          inf: 1e9
        template_column_wise_attention:
          c_in: ${model.node_embed_size}
          c_hidden: 64
          no_heads : 4
      
  ipa:
    sidechain: ${model.sidechain}
    c_s: ${model.node_embed_size}
    c_z: ${model.edge_embed_size}
    c_hidden: 256
    c_skip: 64
    no_heads: 8
    no_qk_points: 8
    no_v_points: 12
    seq_tfmr_num_heads: 4
    seq_tfmr_num_layers: 2
    num_blocks: 4
    coordinate_scaling: ${diffuser.r3.coordinate_scaling}

experiment:
  # Experiment metadata
  name: baseline
  run_id: null

  #training mode
  use_ddp : False

  # Training arguments
  log_freq: 1000
  batch_size: 128
  eval_batch_size: ${data.samples_per_eval_length}
  num_loader_workers: 5
  num_epoch: 20
  learning_rate: 0.0001
  max_squared_res: 300000
  prefetch_factor: 100
  use_gpu: True
  num_gpus: 2
  # seed everything
  seed : 123456

  # Wandb logging
  wandb_dir: ./wandb_runs/
  use_wandb: False

  # How many steps to checkpoint between.
  ckpt_freq: 10000
  # Take early checkpoint at step 100. Helpful for catching eval bugs early.
  early_ckpt: True

  # Checkpoint directory to warm start from.
  warm_start: null
  use_warm_start_conf: False
  ckpt_dir: ./ckpt/

  # Loss weights.
  trans_loss_weight: 1.0
  rot_loss_weight: 1.0
  trans_x0_threshold: 1.0
  coordinate_scaling: ${diffuser.r3.coordinate_scaling}
  bb_atom_loss_weight: 1.0
  bb_atom_loss_t_filter: 0.25
  dist_mat_loss_weight: 1.0
  dist_mat_loss_t_filter: 0.25
  aux_loss_weight: 0.25
  fape_loss_weight: 1.0
  fape_loss_t_filter: 0.0

  # Evaluation.
  eval_dir: ./eval_outputs
  noise_scale: 1.0
  # Filled in during training.
  num_parameters: null

hydra:
  sweeper:
    params:
      # Example of hydra multi run and wandb.
      experiment.name: use_wandb
      experiment.use_wandb: True

"""Script for preprocessing mmcif files for faster consumption.

- Parses all mmcif protein files in a directory.
- Filters out low resolution files.
- Performs any additional processing.
- Writes all processed examples out to specified path.
"""
import tree
import argparse
import dataclasses
import functools as fn
import multiprocessing as mp

import os
import time
import pickle
import logging
import glob
from copy import deepcopy
import mdtraj as md
import numpy as np
import pandas as pd
import torch

from Bio import PDB
from Bio.Seq import Seq
from Bio import pairwise2

from tqdm import tqdm
import torch.multiprocessing

from structure_prediction.data import templates
# amazing! add this line work, without this line, OSError: [Errno 24] Too many open files just at processor start
# and i dont know why
torch.multiprocessing.set_sharing_strategy('file_system')
from data import errors, mmcif_parsing, parsers
from data import utils as du
from data import residue_constants
from data.utils import quaternary_category

logging.basicConfig(level=logging.INFO,
    # define this things every time set basic config
    format="[%(asctime)s] %(levelname)s [%(name)s.%(funcName)s:%(lineno)d] %(message)s",
    datefmt="%d/%b/%Y %H:%M:%S",
    handlers=[
        logging.StreamHandler(),
    ])

base_dir = os.path.dirname(os.path.abspath(__file__))+'/'
# Define the parser
parser = argparse.ArgumentParser(
    description='mmCIF processing script.')
parser.add_argument(
    '--mmcif_dir',
    help='Path to directory with mmcif files.',
    type=str,
    default= base_dir+"../../data/mmCIF/")
parser.add_argument(
    '--data_dir',
    help='Path to directory with data(.pdb or .cif) need to be processed.',
    type=str,
    default= base_dir+"../../data/mmCIF/")
parser.add_argument(
    '--mmcif_cache',
    help='Path to cache metadata.csv , we use it to read the release date of this cif,generated by se3_diffusion/dada/process_pdb_dataset.py',
    type=str,
    default= base_dir+"../../data/processed_pdb/metadata.csv")
parser.add_argument(
    '--processed_name',
    help='the file name of processed pkl',
    type=str,
    default="processed")
parser.add_argument(
    '--alignment_dir',
    help='Path to directory with mmcif files.',
    type=str,
    # default= base_dir+"OpenProteinSet/pdb/"
    default=None)
parser.add_argument(
    '--alignment_index',
    help='Path to precompute alignment database index',
    default = None,
    type=str)
parser.add_argument(
    '--kalign_binary_path',
    help='Path to directory with mmcif files.',
    type=str,
    default="/home/caolongxingLab/wangchentong/bin/kalign")
parser.add_argument(
    '--obsolete_pdbs_file_path',
    help='Path to directory with mmcif files.',
    type=str,
    default=base_dir+"./mmCIF/obsolete.dat")
parser.add_argument(
    '--cluster_file',
    help='Path to cluster file like pdb40 or some other, help to accelerate extract feature, drop protein not in this file.',
    type=str,
    # default=base_dir+'./mmCIF/pdb40.txt'
    default= None
    )
parser.add_argument(
    '--dir_format',
    help='''where each file is writen to, 
    code[1:3] : wirte feature to dir of middle code(2,3) of 4 letter pdb code(follow standard framediff dataprocess),
    code_chain : write feature to dir of code_chain, follow openfold dataset(OpenProteinSet)
    code : write feature to dir of code, follow AFDB
    ''',
    type=str,
    default="code_chain",choices=["code[1:3]","code","code_chain"])
parser.add_argument(
    '--max_file_size',
    help='Max file size.',
    type=int,
    default=3000000000)  # thin constrain is nonsense
parser.add_argument(
    '--min_file_size',
    help='Min file size.',
    type=int,
    default=1)  # Files must be at least 1KB.
parser.add_argument(
    '--max_resolution',
    help='Max resolution of files.',
    type=float,
    default=5.0)
parser.add_argument(
    '--max_len',
    help='Max length of protein.',
    type=int,
    default=512)
parser.add_argument(
    '--min_len',
    help='Min length of protein.',
    type=int,
    default=60)
parser.add_argument(
    '--num_processes',
    help='Number of processes.if gpu is used, this is the number of gpus',
    type=int,
    default=8)
parser.add_argument(
    '--write_dir',
    help='Path to write results to.',
    type=str,
    default=base_dir+'./processed_pdb')
parser.add_argument(
    '--debug',
    help='Turn on for debugging.',
    action='store_true')
parser.add_argument(
    '--verbose',
    help='Whether to log everything.',
    action='store_true',
    default=False)

args = parser.parse_args()

if args.cluster_file and os.path.isfile(args.cluster_file):
    valid_files = set([ name  for line in open(args.cluster_file,"r").readlines() for name in line.strip().split()])
    # Special logic for the mismatch of cluster_file(code_chain) and mmcif_file (code)
else:
    valid_files=set()
    args.cluster_file = None

logging.getLogger("__main__").setLevel(logging.INFO)

if not args.debug:
    logging.getLogger('structure_prediction.data.templates').setLevel(logging.ERROR)
    logging.getLogger(__name__).setLevel(logging.ERROR)

if args.alignment_dir:
    template_featurizer = templates.TemplateHitFeaturizer(
        mmcif_dir=args.mmcif_dir,
        max_template_date='2021-12-1',
        # keep same number of shuffle_top_k in base.config
        max_hits=20,
        kalign_binary_path=args.kalign_binary_path,
        # release_dates_path=template_release_dates_cache_path,
        obsolete_pdbs_path=args.obsolete_pdbs_file_path,
        release_dates_path = args.mmcif_cache
    )

def _retrieve_mmcif_files(
        mmcif_dir: str, max_file_size: int, min_file_size: int, debug: bool):
    """Set up all the mmcif files to read."""
    print('Gathering mmCIF paths')
    valid_files_retrieve = valid_files | set([ name.split('_')[0] for name in valid_files if "_" in name])
    total_num_files = 0
    all_mmcif_paths = []
    mmcif_dir = mmcif_dir
    for subdir in tqdm(os.listdir(mmcif_dir)):
        mmcif_file_dir = os.path.join(mmcif_dir, subdir)
        if not os.path.isdir(mmcif_file_dir):
            if mmcif_file_dir.endswith((".cif",".pdb")) :
                if args.cluster_file:
                    if mmcif_file_dir.replace(".cif","").replace(".pdb","") not in valid_files_retrieve:
                        continue
                total_num_files += 1
                if min_file_size <= os.path.getsize(mmcif_file_dir) <= max_file_size:
                    all_mmcif_paths.append(mmcif_file_dir)
            continue
        for mmcif_file in os.listdir(mmcif_file_dir):
            if not mmcif_file.endswith((".cif",".pdb")):
                continue
            if args.cluster_file:
                if mmcif_file.replace(".cif","").replace(".pdb","") not in valid_files_retrieve:
                    continue
            mmcif_path = os.path.join(mmcif_file_dir, mmcif_file)
            total_num_files += 1
            if min_file_size <= os.path.getsize(mmcif_path) <= max_file_size:
                all_mmcif_paths.append(mmcif_path)
        if debug and total_num_files >= 100:
            all_mmcif_paths = all_mmcif_paths[:100]
            # Don't process all files for debugging
            break
    print(
        f'Processing {len(all_mmcif_paths)} files of {total_num_files}')
    return all_mmcif_paths


def process_mmcif(
        mmcif_path: str, max_resolution: int, write_dir: str):
    """Processes MMCIF files into usable, smaller pickles.

    Args:
        mmcif_path: Path to mmcif file to read.
        max_resolution: Max resolution to allow.
        max_len: Max length to allow.
        write_dir: Directory to write pickles to.

    Returns:
        Saves processed protein to pickle and returns metadata.

    Raises:
        DataError if a known filtering rule is hit.
        All other errors are unexpected and are propogated.
    """
    start_time = time.time()
    metadatas = []
    metadata = {
        "name" : None,
        "release_date" : None,
        "resolution" :None,
        "structure_method" : None,
    }
    struct_chains = {}
    if mmcif_path.endswith('.cif'):

        mmcif_name = os.path.basename(mmcif_path).replace('.cif', '')
        with open(mmcif_path, 'r') as f:
            parsed_mmcif = mmcif_parsing.parse(
                file_id=mmcif_name, mmcif_string=f.read())
        if parsed_mmcif.errors:
            raise errors.MmcifParsingError(
                f'Encountered errors {parsed_mmcif.errors}'
            )
        
        parsed_mmcif = parsed_mmcif.mmcif_object
        mmcif_header = parsed_mmcif.header
        
        release_date = mmcif_header["release_date"]
        mmcif_header = parsed_mmcif.header
        mmcif_resolution = mmcif_header['resolution']

        metadata["name"] = mmcif_name
        metadata["release_date"] = mmcif_header["release_date"]
        metadata['resolution'] = mmcif_resolution
        metadata['structure_method'] = mmcif_header['structure_method']

        struct_chains = {
            chain.id: chain
            for chain in parsed_mmcif.structure.get_chains()}
        
    elif mmcif_path.endswith('.pdb'):
        mmcif_name = os.path.basename(mmcif_path).replace('.pdb', '')
        parser = PDB.PDBParser(QUIET=True)
        structure = parser.get_structure(mmcif_name, mmcif_path)
        structure = [model for model in structure.get_models()][0]
        struct_chains = {
            chain.id: chain
            for chain in structure.get_chains()
        }

    # Extract features
    struct_feats = []
    error_list = []
    for chain_id, chain in struct_chains.items():
        if chain_id.strip()=='':
            chain_id = 'A'
        chain_prot = parsers.process_chain(chain, chain_id)
        chain_dict = dataclasses.asdict(chain_prot)
        chain_dict = du.parse_chain_feats(chain_dict)
        modeled_idx = np.where(chain_dict['aatype'] != 20)[0]
        if modeled_idx.shape[0] == 0:
            logging.info(f"{mmcif_path} {chain_id} has no modeled residues")
            continue
        min_idx = np.min(modeled_idx)
        max_idx = np.max(modeled_idx)
        chain_dict = tree.map_structure(
            lambda x: x[min_idx:(max_idx+1)], chain_dict)
        chain_dict['modeled_idx'] = modeled_idx
        metadata_chain = deepcopy(metadata)
        metadata_chain['name'] =f'{mmcif_name}_{chain_id}'

        metadata_chain = deepcopy(metadata)
        metadata_chain['name'] =f'{mmcif_name}_{chain_id}'
        metadata_chain['modeled_seq_len'] = max_idx - min_idx + 1
        mmcif_subdir = os.path.join(write_dir, f'{mmcif_name}_{chain_id}')
        os.makedirs(mmcif_subdir,exist_ok=True)
        processed_mmcif_path = os.path.join(mmcif_subdir, f'{args.processed_name}.pkl')
        processed_mmcif_path = os.path.abspath(processed_mmcif_path)
        metadata_chain["processed_path"] = processed_mmcif_path

        sequence = "".join([residue_constants.restypes_with_x[index] for index in chain_dict["aatype"]])
        pdb_code = mmcif_name if '_' not in mmcif_name else mmcif_name.split('_')[0]

        if args.alignment_dir:
            alignment_dir = None
            if os.path.exists(os.path.join(args.alignment_dir +f'{mmcif_name}_{chain_id}')):
                alignment_dir = os.path.join(args.alignment_dir +f'{mmcif_name}_{chain_id}/')
            elif os.path.exists(os.path.join(args.alignment_dir +f'{mmcif_name}')):
                alignment_dir = os.path.join(args.alignment_dir +f'{mmcif_name}')
            else:
                logging.warning(f"alignment dir {args.alignment_dir} of {mmcif_name} do not exist")
                continue
            if len(glob.glob(os.path.join(alignment_dir, '*.hhr')))>0:
                pass
            elif len(glob.glob(os.path.join(os.path.join(alignment_dir),"*/*.hhr")))>0:
                hhr_dir = glob.glob(os.path.join(os.path.join(alignment_dir),"*/*.hhr"))[0]
                alignment_dir = os.path.join(hhr_dir,os.path.pardir)
            else:
                logging.warning(f"alignment dir {alignment_dir} do not contain {mmcif_name} hhr!")

            if args.cluster_file:
                if not any([name in valid_files for name in [mmcif_name,f'{mmcif_name}_{chain_id}']]):
                    continue

            hits = templates.parse_template_hits(alignment_dir, args.alignment_index)
            if not any(file.endswith('.hhr') for file in os.listdir(alignment_dir)):
                raise ValueError(f"alignment dir {alignment_dir} do not contain hhr!")
            template_features = templates.make_template_features(
                sequence,
                hits,
                template_featurizer,
                query_release_date=metadata_chain["release_date"],
                query_pdb_code = pdb_code
            )
            if template_features["template_aatype"].shape[0] == 0:
                # all template could be out of date(case:1upg_A)
                logging.getLogger(__name__).warning(f'{mmcif_name}_{chain_id} doest not have proper template!')
            else:
                for key in ["template_sequence","template_domain_names"]:
                    metadata_chain[key] = template_features[key]
                    del template_features[key]
                del metadata_chain["template_sequence"]
            chain_dict.update(template_features)

        metadatas.append(metadata_chain)
        pickle.dump(chain_dict, open(processed_mmcif_path, 'wb'))
        struct_feats.append(chain_dict)

    if error_list.__len__()!=0:
        print(errors.LengthError("\n".join(error_list))) 

    # create folder with different format
    if args.dir_format == "code[1:3]":
        mmcif_subdir = os.path.join(write_dir, mmcif_name[1:3].lower())
        if not os.path.isdir(mmcif_subdir):
            os.mkdir(mmcif_subdir)
    elif args.dir_format == "code":
        mmcif_subdir = os.path.join(write_dir, mmcif_name)
        if not os.path.isdir(mmcif_subdir):
            os.mkdir(mmcif_subdir)

    elapsed_time = time.time() - start_time

    logging.getLogger("__main__").info(f'Finished {mmcif_path} in {elapsed_time:2.2f}s')
    
    return metadatas


def process_serially(
        all_mmcif_paths, max_resolution,  write_dir):
    succeed_num = 0
    metadatas = []
    for i, mmcif_path in enumerate(all_mmcif_paths):
        try:
            start_time = time.time()
            metadatas.append(process_mmcif(
                mmcif_path,
                max_resolution,
                write_dir))
            elapsed_time = time.time() - start_time
            succeed_num+=1
            print(f'Finished {mmcif_path} in {elapsed_time:2.2f}s')
        except errors.DataError as e:
            print(f'Failed {mmcif_path}: {e}')
    return metadatas


def process_fn(
        mmcif_path,
        verbose=None,
        max_resolution=None,
        write_dir=None,):
    try:
        start_time = time.time()
        metadatas = process_mmcif(
            mmcif_path,
            max_resolution,
            write_dir)
        elapsed_time = time.time() - start_time
        if verbose:
            print(f'Finished {mmcif_path} in {elapsed_time:2.2f}s')
        return metadatas
    except Exception as e:
        if verbose:
            print(f'Failed {mmcif_path}: {e}')

def process_fn_list(
    mmcif_paths,**kwargs):
    for mmcif_path in mmcif_paths:
        process_fn(mmcif_path, **kwargs)
def main(args):

    all_mmcif_paths = _retrieve_mmcif_files(
    args.data_dir, args.max_file_size, args.min_file_size, args.debug)
    total_num_paths = len(all_mmcif_paths)
    write_dir = args.write_dir
    if not os.path.exists(write_dir):
        os.makedirs(write_dir)
    if args.debug:
        metadata_file_name = 'metadata_debug.csv'
    else:
        metadata_file_name = 'metadata.csv'
    metadata_path = os.path.join(write_dir, metadata_file_name)
    print(f'Files will be written to {write_dir}')

    # Process each mmcif file
    if args.num_processes == 1 or args.debug:
        all_metadata = process_serially(
            all_mmcif_paths,
            max_resolution=args.max_resolution,
            write_dir=write_dir)
    else:
        #this weird shit happens when kalign use subprocess.Popen and we use mp, figure it out, but for now it's fine with spawn
        _process_fn = fn.partial(
            process_fn,
            verbose=args.verbose,
            max_resolution=args.max_resolution,
            write_dir=write_dir)
        # Uses max number of available cores.
        with mp.Pool() as pool:
            all_metadata = pool.map(_process_fn, all_mmcif_paths)

    all_metadata = [ x for x in all_metadata if x is not None]
    succeeded = len(all_metadata)
    flattened_list = [item for sublist in all_metadata if isinstance(sublist, list) for item in sublist]
    non_list_items = [item for item in all_metadata if not isinstance(item, list)]
    all_metadata = flattened_list+non_list_items
    metadata_df = pd.DataFrame(all_metadata)
    metadata_df.to_csv(metadata_path, index=False)
    print(
        f'Finished processing {succeeded}/{total_num_paths} files')


if __name__ == "__main__":
    # use GPU if available
    # os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    # os.environ["CUDA_VISIBLE_DEVICES"] = ""
    if not os.path.exists(args.mmcif_cache):
        logging.getLogger(__name__).warning(f"mmcif_cache {args.mmcif_cache} not found, template featurizing will be slow")
        args.mmcif_cache = None
    
    mp.set_start_method('spawn')
    args = parser.parse_args()

    main(args)